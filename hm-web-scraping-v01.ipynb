{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Requesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Home Page Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all products url\n",
    "url = 'https://www2.hm.com/en_us/men/products/jeans.html'\n",
    "\n",
    "# headers for request\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0'}\n",
    "\n",
    "# requesting\n",
    "page = requests.get(url=url, headers=headers)\n",
    "\n",
    "# instatiating bs4 object\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding load more products element\n",
    "p = soup.find('div', class_='load-more-products')\n",
    "\n",
    "# all products\n",
    "all_products = int(p.find('h2').get('data-total'))\n",
    "\n",
    "# products per page\n",
    "products_per_page = int(p.find('h2').get('data-items-shown'))\n",
    "\n",
    "# rounding up numer of pages needed for web scraping\n",
    "total_pages = np.ceil(all_products/products_per_page)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  All products in Home Page Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a page with all products\n",
    "url_all_prods = url + '?&offset=0&page-size={}'.format(int(total_pages*products_per_page))\n",
    "\n",
    "all_prods = requests.get(url = url_all_prods, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(all_prods.text, 'html.parser')#.get('li', class_='product-item')\n",
    "\n",
    "# soup.find('li', class_ = 'product-item').find('a').get('href') #.get('item-link')  #.get('item-link') #, class_ = 'item-link')\n",
    "# all find all products listed in homepage\n",
    "products = soup.find_all('li', class_='product-item')\n",
    "\n",
    "# get link to all projects\n",
    "home_links = ['https://www2.hm.com' + link.find('a').get('href') for link in products ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  All products in Each Product Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resulting list of all products to scrap\n",
    "links = []\n",
    "\n",
    "for link in home_links[:3]:\n",
    "    # scrap each product in home page list\n",
    "    single_product = requests.get(link, headers = headers)\n",
    "    soup = BeautifulSoup(single_product.text, 'html.parser')\n",
    "\n",
    "    # gets the links to all products listed in a page\n",
    "    products_ul = soup.find('ul', class_='inputlist clearfix')\n",
    "    products = products_ul.find_all('a')\n",
    "\n",
    "    links_ul = []\n",
    "    links_ul = [ 'https://www2.hm.com' + item.get('href') for item in products]\n",
    "    links.extend(links_ul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e351bda13fd12210c56cd25328508007dc56e1370bfddb216f0f5bba4101204e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('web_scraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
